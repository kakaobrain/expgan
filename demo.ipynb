{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from models.expgan import EXPGAN\n",
    "from dataset.ffhq import FFHQDataset\n",
    "from glob import glob\n",
    "from models.flamedecoder import FlameDecoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "fn_ckpt = './pretrained_model/model_checkpoint.ckpt'\n",
    "fn_cfg = './experiments/ffhq/config.yaml'\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "cfg = OmegaConf.load(fn_cfg)\n",
    "cfg.dataset.fn_meta_flip = None\n",
    "\n",
    "img_wh = (cfg.dataset.image_size, cfg.dataset.image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EXPGAN(cfg)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model.load_from_checkpoint(fn_ckpt)\n",
    "\n",
    "model.net_G_ema.decoder.coarse_steps = cfg.model.EG3D.coarse_steps * 2\n",
    "model.net_G_ema.decoder.fine_steps = cfg.model.EG3D.fine_steps * 2\n",
    "model.net_G_ema.decoder.perturb = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset, FLAME decoder, and an example DECA parameters for demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1\n",
    "dataset = FFHQDataset(**cfg.dataset, split='train')\n",
    "mesh_decoder = FlameDecoder(**cfg.model.flamedecoder, masking=False).cuda()\n",
    "\n",
    "fn_transfer = 'data/demo/meta_smooth.json'\n",
    "transfer_meta = json.load(open(fn_transfer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "steps = 120\n",
    "angles = np.linspace(0, np.pi * 4, steps)\n",
    "angles = [np.sin(angle) / 6 for angle in angles]\n",
    "\n",
    "frames = []\n",
    "\n",
    "for bidx, batch in enumerate(dataloader):\n",
    "    # seed = torch.seed()\n",
    "    # print(seed)\n",
    "    for angle in tqdm(angles):\n",
    "        torch.manual_seed(13138407004106472821)\n",
    "        batch.update({name: tensor.cuda() for name, tensor in batch.items() if type(tensor) == torch.Tensor})\n",
    "        batch['codedict_real'].update({name: tensor.cuda() for name, tensor in batch['codedict_real'].items() if type(tensor) == torch.Tensor})\n",
    "        pose = torch.FloatTensor([[0, angle, 0]]).cuda()\n",
    "\n",
    "        flame = model.flamedecoder(batch['codedict_real'], batch['bbox_real'], pose=pose)\n",
    "        uv = flame['uv']\n",
    "        depth = flame['depth']\n",
    "        c2w = flame['c2w']\n",
    "        shape = batch['shape_real']\n",
    "        batch['codedict_real']['shape']\n",
    "\n",
    "        output = model.net_G_ema(shape, c2w, uv, depth, truncation=0.5, update_mean=False)        \n",
    "        pred = output['pred'] * 0.5 + 0.5\n",
    "\n",
    "        pred = pred.permute(0,2,3,1).detach().cpu()[0] #.numpy()[0]\n",
    "        frames.append(pred)\n",
    "\n",
    "        # plt.figure(figsize=(5,5))\n",
    "        # plt.imshow(pred)\n",
    "\n",
    "    #     break\n",
    "    break\n",
    "\n",
    "torchvision.io.write_video('results/demo_yaw.mp4', torch.stack(frames, dim=0) * 255., fps=30, options={'crf': '15'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expression interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "steps = 120\n",
    "\n",
    "target_frames = [1, 220, 291, 1]\n",
    "target_frames = [str(i) for i in target_frames]\n",
    "target_exps = [torch.FloatTensor(transfer_meta['frames'][key]['exp']).unsqueeze(0) for key in target_frames]\n",
    "target_jaws = [torch.FloatTensor(transfer_meta['frames'][key]['pose'][3:]).unsqueeze(0) for key in target_frames]\n",
    "\n",
    "weights = torch.linspace(0, 1, steps // (len(target_exps)-1)).unsqueeze(1)\n",
    "\n",
    "exps = []\n",
    "for i in range(len(target_exps)-1): \n",
    "    lerp_exp = torch.lerp(target_exps[i], target_exps[i+1], weights)\n",
    "    exps.append(lerp_exp)\n",
    "exps = torch.cat(exps, dim=0).cuda()\n",
    "\n",
    "jaws = []\n",
    "for i in range(len(target_jaws)-1): \n",
    "    lerp_jaw = torch.lerp(target_jaws[i], target_jaws[i+1], weights)\n",
    "    jaws.append(lerp_jaw)\n",
    "jaws = torch.cat(jaws, dim=0).cuda()\n",
    "\n",
    "frames = []\n",
    "\n",
    "for bidx, batch in enumerate(dataloader):\n",
    "    # seed = torch.seed()\n",
    "    # print(seed)\n",
    "    for exp, jaw in tqdm(zip(exps, jaws)):\n",
    "        torch.manual_seed(0)\n",
    "        batch.update({name: tensor.cuda() for name, tensor in batch.items() if type(tensor) == torch.Tensor})\n",
    "        batch['codedict_real'].update({name: tensor.cuda() for name, tensor in batch['codedict_real'].items() if type(tensor) == torch.Tensor})\n",
    "        batch['codedict_real']['exp'] = exp.unsqueeze(0).expand(1, -1)\n",
    "        batch['codedict_real']['pose'][:,3:] = jaw\n",
    "\n",
    "        flame = model.flamedecoder(batch['codedict_real'], batch['bbox_real'], pose=pose)\n",
    "        uv = flame['uv']\n",
    "        depth = flame['depth']\n",
    "        c2w = flame['c2w']\n",
    "        shape = batch['shape_real']\n",
    "        batch['codedict_real']['shape']\n",
    "\n",
    "        output = model.net_G_ema(shape, c2w, uv, depth, truncation=0.5, update_mean=False)        \n",
    "        pred = output['pred'] * 0.5 + 0.5\n",
    "\n",
    "        pred = pred.permute(0,2,3,1).detach().cpu()[0]\n",
    "        frames.append(pred)\n",
    "    break\n",
    "\n",
    "torchvision.io.write_video('results/demo_expr.mp4', torch.stack(frames, 0) * 255., fps=30, options={'crf': '15'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose and expression interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=4)\n",
    "\n",
    "steps = 120\n",
    "angles = np.linspace(0, np.pi * 4, steps)\n",
    "\n",
    "x_angles = [np.sin(angle) / 6 for angle in angles]\n",
    "y_angles = [np.cos(angle) / 6 for angle in angles]\n",
    "\n",
    "target_frames = [1, 220, 150, 291, 1]\n",
    "target_frames = [str(i) for i in target_frames]\n",
    "target_exps = [torch.FloatTensor(transfer_meta['frames'][key]['exp']).unsqueeze(0) for key in target_frames]\n",
    "target_jaws = [torch.FloatTensor(transfer_meta['frames'][key]['pose'][3:]).unsqueeze(0) for key in target_frames]\n",
    "\n",
    "weights = torch.linspace(0, 1, steps // (len(target_exps)-1)).unsqueeze(1)\n",
    "\n",
    "exps = []\n",
    "for i in range(len(target_exps)-1): \n",
    "    lerp_exp = torch.lerp(target_exps[i], target_exps[i+1], weights)\n",
    "    exps.append(lerp_exp)\n",
    "exps = torch.cat(exps, dim=0).cuda()\n",
    "\n",
    "jaws = []\n",
    "for i in range(len(target_jaws)-1): \n",
    "    lerp_jaw = torch.lerp(target_jaws[i], target_jaws[i+1], weights)\n",
    "    jaws.append(lerp_jaw)\n",
    "jaws = torch.cat(jaws, dim=0).cuda()\n",
    "\n",
    "\n",
    "frames = []\n",
    "\n",
    "for bidx, batch in enumerate(dataloader) :\n",
    "    seed = torch.seed()\n",
    "    for x_angle, y_angle, exp, jaw in tqdm(zip(x_angles, y_angles, exps, jaws)) :\n",
    "        # seed = bidx \n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        batch.update({name: tensor.cuda() for name, tensor in batch.items() if type(tensor) == torch.Tensor})\n",
    "        batch['codedict_real'].update({name: tensor.cuda() for name, tensor in batch['codedict_real'].items() if type(tensor) == torch.Tensor})\n",
    "        pose = torch.FloatTensor([[x_angle, y_angle, 0]]).cuda().expand(4, -1)\n",
    "        batch['codedict_real']['exp'] = exp.unsqueeze(0).expand(4, -1)\n",
    "        batch['codedict_real']['pose'][:,3:] = jaw\n",
    "        \n",
    "        flame = model.flamedecoder(batch['codedict_real'], batch['bbox_real'], pose=pose)\n",
    "        uv = flame['uv']\n",
    "        depth = flame['depth']\n",
    "        c2w = flame['c2w']\n",
    "        shape = batch['shape_real']\n",
    "        batch['codedict_real']['shape']\n",
    "        \n",
    "        flame_fullhead = mesh_decoder(batch['codedict_real'], batch['bbox_real'], pose=pose, mesh_render=True)\n",
    "        shape_image = torch.clip(flame_fullhead['shape_image'][0].permute(1,2,0), 0, 1).cpu() #.numpy()\n",
    "\n",
    "        output = model.net_G_ema(shape, c2w, uv, depth, truncation=0.5, update_mean=False)        \n",
    "        pred = output['pred'] * 0.5 + 0.5\n",
    "\n",
    "        pred = pred.permute(0,2,3,1).detach().cpu() #.numpy()\n",
    "        top = torch.cat([shape_image, pred[0], pred[1]], dim=1)\n",
    "        bot = torch.cat([torch.ones_like(shape_image), pred[2], pred[3]], dim=1)\n",
    "        pred = torch.cat([top, bot], dim=0)\n",
    "\n",
    "        frames.append(pred)\n",
    "    break\n",
    "\n",
    "torchvision.io.write_video('results/demo_pose_expr.mp4', torch.stack(frames, 0) * 255., fps=30, options={'crf': '15'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low- and high-resolution results before and after StyleGAN upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "steps = 120\n",
    "angles = np.linspace(0, np.pi * 4, steps)\n",
    "\n",
    "x_angles = [np.sin(angle) / 6 for angle in angles]\n",
    "y_angles = [np.cos(angle) / 6 for angle in angles]\n",
    "\n",
    "target_frames = [1, 220, 150, 291, 1]\n",
    "target_frames = [str(i) for i in target_frames]\n",
    "target_exps = [torch.FloatTensor(transfer_meta['frames'][key]['exp']).unsqueeze(0) for key in target_frames]\n",
    "target_jaws = [torch.FloatTensor(transfer_meta['frames'][key]['pose'][3:]).unsqueeze(0) for key in target_frames]\n",
    "\n",
    "weights = torch.linspace(0, 1, steps // (len(target_exps)-1)).unsqueeze(1)\n",
    "\n",
    "exps = []\n",
    "for i in range(len(target_exps)-1): \n",
    "    lerp_exp = torch.lerp(target_exps[i], target_exps[i+1], weights)\n",
    "    exps.append(lerp_exp)\n",
    "exps = torch.cat(exps, dim=0).cuda()\n",
    "\n",
    "jaws = []\n",
    "for i in range(len(target_jaws)-1): \n",
    "    lerp_jaw = torch.lerp(target_jaws[i], target_jaws[i+1], weights)\n",
    "    jaws.append(lerp_jaw)\n",
    "jaws = torch.cat(jaws, dim=0).cuda()\n",
    "\n",
    "frames = []\n",
    "\n",
    "for bidx, batch in enumerate(dataloader) :\n",
    "    seed = torch.seed()\n",
    "    for x_angle, y_angle, exp, jaw in tqdm(zip(x_angles, y_angles, exps, jaws)) :\n",
    "        # seed = bidx \n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        batch.update({name: tensor.cuda() for name, tensor in batch.items() if type(tensor) == torch.Tensor})\n",
    "        batch['codedict_real'].update({name: tensor.cuda() for name, tensor in batch['codedict_real'].items() if type(tensor) == torch.Tensor})\n",
    "        pose = torch.FloatTensor([[x_angle, y_angle, 0]]).cuda().expand(1, -1)\n",
    "        batch['codedict_real']['exp'] = exp.unsqueeze(0).expand(1, -1)\n",
    "        batch['codedict_real']['pose'][:,3:] = jaw\n",
    "        \n",
    "        flame = model.flamedecoder(batch['codedict_real'], batch['bbox_real'], pose=pose)\n",
    "        uv = flame['uv']\n",
    "        depth = flame['depth']\n",
    "        c2w = flame['c2w']\n",
    "        shape = batch['shape_real']\n",
    "        batch['codedict_real']['shape']\n",
    "        \n",
    "        flame_fullhead = mesh_decoder(batch['codedict_real'], batch['bbox_real'], pose=pose, mesh_render=True)\n",
    "        shape_image = torch.clip(flame_fullhead['shape_image'][0].permute(1,2,0), 0, 1).cpu()\n",
    "\n",
    "        output = model.net_G_ema(shape, c2w, uv, depth, truncation=0.5, update_mean=False)   \n",
    "        pred = output['pred'] * 0.5 + 0.5\n",
    "        pred_low = output['aux_out'] * 0.5 + 0.5\n",
    "\n",
    "        pred = pred.permute(0,2,3,1).detach().cpu()\n",
    "        pred_low = torch.nn.functional.interpolate(pred_low, size=(256,256), mode='bilinear')\n",
    "        pred_low = pred_low.permute(0,2,3,1).detach().cpu()\n",
    "        # pred_low = np.pad(pred_low[0], ((96,96), (96,96), (0,0)), 'constant', constant_values=0)\n",
    "        vis = torch.cat([shape_image, pred_low[0], pred[0]], dim=1)\n",
    "\n",
    "        frames.append(vis)\n",
    "    break\n",
    "\n",
    "torchvision.io.write_video('results/demo_low_high_res.mp4', torch.stack(frames, 0) * 255., fps=30, options={'crf': '15'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts for figures\n",
    "\n",
    "### Pose interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_angles = np.linspace(-0.9, 0.9, 7)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "\n",
    "for bidx, batch in enumerate(dataloader):\n",
    "    seed = torch.seed()\n",
    "    frames = []\n",
    "    for angle in y_angles:\n",
    "        torch.manual_seed(100)\n",
    "        batch.update({name: tensor.cuda() for name, tensor in batch.items() if type(tensor) == torch.Tensor})\n",
    "        batch['codedict_real'].update({name: tensor.cuda() for name, tensor in batch['codedict_real'].items() if type(tensor) == torch.Tensor})\n",
    "\n",
    "        pose = torch.FloatTensor([[0, angle, 0]]).cuda()\n",
    "        # pose = None\n",
    "\n",
    "        flame = model.flamedecoder(batch['codedict_real'], batch['bbox_real'], pose=pose, mesh_render=True)\n",
    "        uv = flame['uv']\n",
    "        depth = flame['depth']\n",
    "        c2w = flame['c2w']\n",
    "        shape = batch['shape_real']\n",
    "        shape_image = torch.clip(flame['shape_image'][0].permute(1,2,0), 0, 1).cpu().numpy()\n",
    "\n",
    "        output = model.net_G_ema(shape, c2w, uv, depth, truncation=0.5, update_mean=False)        \n",
    "        pred = output['pred'] * 0.5 + 0.5\n",
    "        pred = pred[0].permute(1,2,0).detach().cpu().numpy()\n",
    "        frames.append(pred)\n",
    "\n",
    "    pred = np.concatenate(frames, axis=1)\n",
    "    cv2.imwrite(f'results/pose_interp.png', (pred * 255).round().astype(np.uint8)[:,:,::-1])\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(pred)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expression interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 7\n",
    "weights = torch.linspace(0, 1, steps).unsqueeze(1)\n",
    "\n",
    "rand_1 = 170 # np.random.randint(len(dataset))\n",
    "rand_2 = 41788 # np.random.randint(len(dataset))\n",
    "exp1 = dataset[rand_1]['codedict_real']['exp'][None,...]\n",
    "exp2 = dataset[rand_2]['codedict_real']['exp'][None,...]\n",
    "jaw1 = dataset[rand_1]['codedict_real']['pose'][3:][None,...]\n",
    "jaw2 = dataset[rand_2]['codedict_real']['pose'][3:][None,...]\n",
    "\n",
    "print(rand_1, rand_2)\n",
    "\n",
    "exps = torch.lerp(exp1, exp2, weights).cuda()\n",
    "jaws = torch.lerp(jaw1, jaw2, weights).cuda()\n",
    "\n",
    "for bidx, batch in enumerate(dataloader):\n",
    "    seed = torch.seed()\n",
    "    frames = []\n",
    "    for exp, jaw in zip(exps, jaws):\n",
    "        torch.manual_seed(seed)\n",
    "        batch.update({name: tensor.cuda() for name, tensor in batch.items() if type(tensor) == torch.Tensor})\n",
    "        batch['codedict_real'].update({name: tensor.cuda() for name, tensor in batch['codedict_real'].items() if type(tensor) == torch.Tensor})\n",
    "\n",
    "        batch['codedict_real']['exp'] = exp.unsqueeze(0)\n",
    "        batch['codedict_real']['pose'][:,3:] = jaw\n",
    "        batch['shape_real'][:,100:] = jaw\n",
    "        pose = torch.FloatTensor([[0,0,0]]).cuda()\n",
    "\n",
    "        flame = model.flamedecoder(batch['codedict_real'], batch['bbox_real'], pose=pose, mesh_render=True)\n",
    "        uv = flame['uv']\n",
    "        depth = flame['depth']\n",
    "        c2w = flame['c2w']\n",
    "        shape = batch['shape_real']\n",
    "        shape_image = torch.clip(flame['shape_image'][0].permute(1,2,0), 0, 1).cpu().numpy()\n",
    "\n",
    "        output = model.net_G_ema(shape, c2w, uv, depth, truncation=0.5, update_mean=False)        \n",
    "        pred = output['pred'] * 0.5 + 0.5\n",
    "        pred = pred[0].permute(1,2,0).detach().cpu().numpy()\n",
    "        frames.append(pred)\n",
    "\n",
    "    pred = np.concatenate(frames, axis=1)\n",
    "    cv2.imwrite(f'results/expression_interp.png', (pred * 255).round().astype(np.uint8)[:,:,::-1])\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(pred)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 7\n",
    "weights = torch.linspace(0, 1, steps).unsqueeze(1)\n",
    "\n",
    "rand_1 = 2132 # np.random.randint(len(dataset))\n",
    "rand_2 = 15 # np.random.randint(len(dataset))\n",
    "shape1 = dataset[rand_1]['codedict_real']['shape'][None,...]\n",
    "shape2 = dataset[rand_2]['codedict_real']['shape'][None,...]\n",
    "\n",
    "print(rand_1, rand_2)\n",
    "\n",
    "shapes = torch.lerp(shape1, shape2, weights).cuda()\n",
    "\n",
    "for bidx, batch in enumerate(dataloader):\n",
    "    seed = torch.seed()\n",
    "    frames = []\n",
    "    for shape in shapes:\n",
    "        torch.manual_seed(seed)\n",
    "        batch.update({name: tensor.cuda() for name, tensor in batch.items() if type(tensor) == torch.Tensor})\n",
    "        batch['codedict_real'].update({name: tensor.cuda() for name, tensor in batch['codedict_real'].items() if type(tensor) == torch.Tensor})\n",
    "\n",
    "        batch['codedict_real']['shape'] = shape.unsqueeze(0)\n",
    "        batch['shape_real'][:,:100] = shape\n",
    "        pose = torch.FloatTensor([[0,0,0]]).cuda()\n",
    "\n",
    "        flame = model.flamedecoder(batch['codedict_real'], batch['bbox_real'], pose=pose, mesh_render=True)\n",
    "        uv = flame['uv']\n",
    "        depth = flame['depth']\n",
    "        c2w = flame['c2w']\n",
    "        shape = batch['shape_real']\n",
    "        shape_image = torch.clip(flame['shape_image'][0].permute(1,2,0), 0, 1).cpu().numpy()\n",
    "\n",
    "        output = model.net_G_ema(shape, c2w, uv, depth, truncation=0.5, update_mean=False)        \n",
    "        pred = output['pred'] * 0.5 + 0.5\n",
    "        pred = pred[0].permute(1,2,0).detach().cpu().numpy()\n",
    "        frames.append(pred)\n",
    "        # frames.append(shape_image)\n",
    "\n",
    "        # plt.imshow(pred)\n",
    "        # break\n",
    "\n",
    "    pred = np.concatenate(frames, axis=1)\n",
    "    cv2.imwrite(f'results/shape_interp.png', (pred * 255).round().astype(np.uint8)[:,:,::-1])\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(pred)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent vector (w space) interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = model.net_G_ema.z_dim\n",
    "\n",
    "steps = 7\n",
    "weights = torch.linspace(0, 1, steps).unsqueeze(1).cuda()\n",
    "\n",
    "rand_1 = np.random.randint(len(dataset))\n",
    "rand_2 = np.random.randint(len(dataset))\n",
    "\n",
    "torch.manual_seed(rand_1)\n",
    "latent1 = torch.randn(1, z_dim)\n",
    "torch.manual_seed(rand_2)\n",
    "latent2 = torch.randn(1, z_dim)\n",
    "\n",
    "for bidx, batch in enumerate(dataloader):\n",
    "    shape = batch['shape_real']\n",
    "\n",
    "    rand_1 = np.random.randint(len(dataset))\n",
    "    rand_2 = np.random.randint(len(dataset))\n",
    "    print(rand_1, rand_2)\n",
    "\n",
    "    torch.manual_seed(rand_1)\n",
    "    latent1 = torch.randn(1, 512)\n",
    "    torch.manual_seed(rand_2)\n",
    "    latent2 = torch.randn(1, 512)\n",
    "\n",
    "    latent1 = torch.cat([latent1, shape], dim=-1).cuda()\n",
    "    latent2 = torch.cat([latent2, shape], dim=-1).cuda()\n",
    "\n",
    "    latent1_w = model.net_G_ema.mapping_network(latent1)\n",
    "    latent2_w = model.net_G_ema.mapping_network(latent2)\n",
    "\n",
    "    latents_w = torch.lerp(latent1_w[:,0], latent2_w[:,0], weights)\n",
    "    latents_w = latents_w.unsqueeze(1).expand(-1, 29, -1)\n",
    "\n",
    "\n",
    "    seed = torch.seed()\n",
    "    frames = []\n",
    "    for latent in latents_w:\n",
    "        torch.manual_seed(seed)\n",
    "        batch.update({name: tensor.cuda() for name, tensor in batch.items() if type(tensor) == torch.Tensor})\n",
    "        batch['codedict_real'].update({name: tensor.cuda() for name, tensor in batch['codedict_real'].items() if type(tensor) == torch.Tensor})\n",
    "\n",
    "        pose = torch.FloatTensor([[0,0,0]]).cuda()\n",
    "\n",
    "        flame = model.flamedecoder(batch['codedict_real'], batch['bbox_real'], pose=pose, mesh_render=True)\n",
    "        uv = flame['uv']\n",
    "        depth = flame['depth']\n",
    "        c2w = flame['c2w']\n",
    "        shape = batch['shape_real']\n",
    "        shape_image = torch.clip(flame['shape_image'][0].permute(1,2,0), 0, 1).cpu().numpy()\n",
    "\n",
    "        output = model.net_G_ema(shape, c2w, uv, depth, truncation=0.5, update_mean=False, w=latent.unsqueeze(0))        \n",
    "        pred = output['pred'] * 0.5 + 0.5\n",
    "        pred = pred[0].permute(1,2,0).detach().cpu().numpy()\n",
    "        frames.append(pred)\n",
    "\n",
    "    pred = np.concatenate(frames, axis=1)\n",
    "    cv2.imwrite(f'results/identity_interp.png', (pred * 255).round().astype(np.uint8)[:,:,::-1])\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(pred)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
